import sys
import os
import time
import requests
import pandas as pd
import io
import random
import datetime
from datetime import datetime as dt
import traceback
import json

# ================= âš™ï¸ ç”¨æˆ·é…ç½® (Webéƒ¨ç½²ç‰ˆ) =================

# 1. é”å®šè„šæœ¬æ‰€åœ¨ç›®å½•
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))

# 2. æ–‡ä»¶è·¯å¾„é…ç½®
# æ³¨æ„ï¼šExcelåœ¨äº‘ç«¯ä¸ä¸€å®šéœ€è¦ï¼Œä½†ä¸ºäº†è°ƒè¯•å¯ä»¥ä¿ç•™
EXCEL_NAME = os.path.join(CURRENT_DIR, "market_data.xlsx")
FINANCIAL_FILE = os.path.join(CURRENT_DIR, 'temp_data_financial.csv')
PRICE_FILE = os.path.join(CURRENT_DIR, 'temp_price_history.csv')
JSON_FILE = os.path.join(CURRENT_DIR, 'data.json') # ã€æ–°å¢ã€‘Webæ•°æ®æº

# çˆ¬è™«ä¼ªè£…
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 Safari/605.1.15"
]

INDICATOR_MAPPING = {
    'åŸºæœ¬æ¯è‚¡æ”¶ç›Š': 'EPS', 'æ¯è‚¡å‡€èµ„äº§': 'BVPS', 'æ¯è‚¡ç»è¥æ´»åŠ¨': 'OCFPS',
    'å‡€èµ„äº§æ”¶ç›Šç‡': 'ROE', 'é”€å”®å‡€åˆ©ç‡': 'å‡€åˆ©ç‡', 'é”€å”®æ¯›åˆ©ç‡': 'æ¯›åˆ©ç‡',
    'è¥ä¸šæ€»æ”¶å…¥': 'è¥æ”¶', 'å‡€åˆ©æ¶¦': 'å‡€åˆ©', 'æ‰£éå‡€åˆ©æ¶¦': 'æ‰£éå‡€åˆ©',
    'èµ„äº§è´Ÿå€ºç‡': 'è´Ÿå€ºç‡', 'æµåŠ¨æ¯”ç‡': 'æµåŠ¨æ¯”', 'é€ŸåŠ¨æ¯”ç‡': 'é€ŸåŠ¨æ¯”',
    'å­˜è´§å‘¨è½¬ç‡': 'å­˜è´§å‘¨è½¬', 'åº”æ”¶è´¦æ¬¾å‘¨è½¬ç‡': 'åº”æ”¶å‘¨è½¬'
}

META_INFO = {
    "ä»£ç ": "æ–‡æœ¬", "åç§°": "æ–‡æœ¬", "æœ€æ–°ä»·": "å…ƒ", "æ¶¨è·Œå¹…%": "%",
    "æ€»å¸‚å€¼(ä¸‡)": "ä¸‡å…ƒ", "å¸‚ç›ˆç‡(åŠ¨)": "å€", "å¸‚å‡€ç‡": "å€", "æ¢æ‰‹ç‡%": "%", "æˆäº¤é¢(ä¸‡)": "ä¸‡å…ƒ",
    "EPS": "å…ƒ", "BVPS": "å…ƒ", "OCFPS": "å…ƒ", "ROE": "%", "å‡€åˆ©ç‡": "%", "æ¯›åˆ©ç‡": "%",
    "è¥æ”¶": "å…ƒ", "å‡€åˆ©": "å…ƒ", "æ‰£éå‡€åˆ©": "å…ƒ", "è´Ÿå€ºç‡": "%", "æµåŠ¨æ¯”": "å€", "é€ŸåŠ¨æ¯”": "å€",
    "å­˜è´§å‘¨è½¬": "æ¬¡", "åº”æ”¶å‘¨è½¬": "æ¬¡"
}

os.environ['http_proxy'] = ''
os.environ['https_proxy'] = ''

# ================= ğŸ› ï¸ å·¥å…·å‡½æ•° =================

def clean_code(x):
    try:
        s = str(x).strip()
        if not s or s == 'è‚¡ç¥¨ä»£ç ' or s.lower() == 'nan' or 'ä»£ç ' in s: return None
        return str(int(float(s))).zfill(6)
    except:
        return s.zfill(6) if s else None

def get_random_header():
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Referer": "https://finance.sina.com.cn/"
    }

def to_wan(x):
    if x == '-' or x is None: return '-'
    try: return round(float(x) / 10000, 2)
    except: return x

def get_sina_symbol(code):
    if code.startswith('6'): return f"sh{code}"
    if code.startswith('0') or code.startswith('3'): return f"sz{code}"
    if code.startswith('8') or code.startswith('4'): return f"bj{code}"
    return f"sz{code}" 

# ================= é˜¶æ®µä¸€ï¼šè¡Œæƒ… + æœˆå‡ä»· =================

def fetch_market_snapshot():
    print(f"\nğŸš€ [é˜¶æ®µä¸€] æ‹‰å–å…¨å¸‚åœºå®æ—¶è¡Œæƒ…...")
    all_dfs = []
    page = 1
    # ä¸ºäº†æ¼”ç¤ºé€Ÿåº¦ï¼Œå¦‚æœæ˜¯åœ¨GitHub Actionsé‡Œï¼Œå¯ä»¥é€‚å½“å¢åŠ å¹¶å‘æˆ–é¡µæ•°
    # è¿™é‡Œä¿æŒç¨³å¥çš„å•çº¿ç¨‹
    while page <= 100: 
        url = "http://vip.stock.finance.sina.com.cn/quotes_service/api/json_v2.php/Market_Center.getHQNodeData"
        params = {"page": str(page), "num": "80", "sort": "changepercent", "asc": "0", "node": "hs_a", "symbol": "", "_s_r_a": "sort"}
        try:
            res = requests.get(url, params=params, headers=get_random_header(), timeout=10)
            if not res.text or res.text == 'null' or res.text == '[]': break
            df = pd.read_json(io.StringIO(res.text), dtype={'code': str})
            if not df.empty: all_dfs.append(df)
            else: break
        except: pass
        page += 1
        time.sleep(0.05)
        
    if not all_dfs: return pd.DataFrame()
    
    full_df = pd.concat(all_dfs, ignore_index=True)
    rename_map = {
        "code": "ä»£ç ", "name": "åç§°", "trade": "æœ€æ–°ä»·", "changepercent": "æ¶¨è·Œå¹…%", 
        "mktcap": "æ€»å¸‚å€¼(ä¸‡)", "per": "å¸‚ç›ˆç‡(åŠ¨)", "pb": "å¸‚å‡€ç‡", "turnoverratio": "æ¢æ‰‹ç‡%", "amount": "æˆäº¤é¢"
    }
    cols = [c for c in rename_map.keys() if c in full_df.columns]
    df_final = full_df[cols].rename(columns=rename_map)
    df_final["ä»£ç "] = df_final["ä»£ç "].apply(clean_code)
    df_final = df_final.dropna(subset=['ä»£ç '])
    
    if "æˆäº¤é¢" in df_final.columns:
        df_final["æˆäº¤é¢(ä¸‡)"] = df_final["æˆäº¤é¢"].apply(to_wan)
        del df_final["æˆäº¤é¢"]
    if "æ€»å¸‚å€¼(ä¸‡)" in df_final.columns:
        df_final["æ€»å¸‚å€¼(ä¸‡)"] = df_final["æ€»å¸‚å€¼(ä¸‡)"].apply(lambda x: round(float(x), 2) if x else '-')

    print(f"   âœ… è·å–åˆ° {len(df_final)} åªè‚¡ç¥¨åŸºç¡€è¡Œæƒ…")
    return df_final

def get_stock_monthly_history(code):
    symbol = get_sina_symbol(code)
    url = f"https://quotes.sina.cn/cn/api/json_v2.php/CN_MarketDataService.getKLineData?symbol={symbol}&scale=240&ma=no&datalen=400"
    try:
        res = requests.get(url, headers=get_random_header(), timeout=5)
        data = res.json()
        if not data: return None
        df = pd.DataFrame(data)
        df['day'] = pd.to_datetime(df['day'])
        df['close'] = df['close'].astype(float)
        df.set_index('day', inplace=True)
        # æŒ‰æœˆè®¡ç®—å‡ä»·
        monthly_df = df['close'].resample('ME').mean().sort_index(ascending=False)
        last_12 = monthly_df.head(12)
        result = {}
        for date, price in last_12.items():
            col_name = f"{date.strftime('%Y-%m')}_å‡ä»·"
            result[col_name] = round(price, 2)
        return result
    except: return None

def augment_with_monthly_prices(market_df):
    print(f"\nğŸ“Š [é˜¶æ®µä¸€Â·è¡¥å……] æ­£åœ¨è®¡ç®—/è¯»å–æœˆåº¦å‡ä»·...")
    cached_prices = pd.DataFrame()
    
    # è¯»å–ç¼“å­˜ (GitHub Action pullä¸‹æ¥çš„æ–‡ä»¶)
    if os.path.exists(PRICE_FILE):
        try:
            cached_prices = pd.read_csv(PRICE_FILE, dtype={'ä»£ç ': str})
            cached_prices['ä»£ç '] = cached_prices['ä»£ç '].apply(clean_code)
            cached_prices = cached_prices.set_index('ä»£ç ')
            print(f"   ğŸ“‚ æˆåŠŸåŠ è½½æœˆä»·ç¼“å­˜: {len(cached_prices)} æ¡")
        except: pass
    
    target_codes = market_df['ä»£ç '].tolist()
    # æ‰¾å‡ºç¼“å­˜é‡Œæ²¡æœ‰çš„è‚¡ç¥¨
    todo_codes = [c for c in target_codes if c not in cached_prices.index]
    
    print(f"   éœ€è¡¥å½•: {len(todo_codes)} åª")

    new_data_list = []
    if todo_codes:
        # ä¸ºäº†é¿å…äº‘ç«¯è¿è¡Œè¶…æ—¶ï¼Œé™åˆ¶æ¯æ¬¡æœ€å¤šè¡¥å½• 500 ä¸ª (æ¯å¤©è·‘ä¸€ç‚¹ï¼Œæ…¢æ…¢å°±å…¨äº†)
        # ç¬¬ä¸€æ¬¡è¿è¡Œä¼šæ¯”è¾ƒä¹…
        limit = 2000 
        print(f"   â³ æœ¬æ¬¡è¿è¡Œé™åˆ¶è¡¥å½• {limit} åªï¼Œé˜²æ­¢è¶…æ—¶...")
        
        for i, code in enumerate(todo_codes[:limit]):
            if i % 50 == 0: print(f"   è¿›åº¦: {i}/{len(todo_codes[:limit])}...", end="\r")
            monthly_data = get_stock_monthly_history(code)
            if monthly_data:
                monthly_data['ä»£ç '] = code
                new_data_list.append(monthly_data)
            time.sleep(0.02)

    if new_data_list:
        new_df = pd.DataFrame(new_data_list)
        new_df.set_index('ä»£ç ', inplace=True)
        if not cached_prices.empty:
            final_cache = pd.concat([cached_prices, new_df])
            final_cache = final_cache[~final_cache.index.duplicated(keep='last')]
        else:
            final_cache = new_df
        final_cache.to_csv(PRICE_FILE, encoding='utf-8-sig')
        cached_prices = final_cache
        print(f"   âœ… æœˆä»·ç¼“å­˜å·²æ›´æ–°å¹¶ä¿å­˜ã€‚")

    market_df = market_df.set_index('ä»£ç ')
    cached_prices = cached_prices.reindex(market_df.index).dropna(how='all')
    market_df = market_df.join(cached_prices)
    market_df = market_df.reset_index()
    return market_df

# ================= é˜¶æ®µäºŒï¼šè´¢åŠ¡æ•°æ®è¡¥å½• =================

def get_existing_financial_codes():
    if not os.path.exists(FINANCIAL_FILE): return set()
    try:
        df = pd.read_csv(FINANCIAL_FILE, dtype=str, on_bad_lines='skip')
        col = 'è‚¡ç¥¨ä»£ç ' if 'è‚¡ç¥¨ä»£ç ' in df.columns else df.columns[1]
        codes = df[col].apply(clean_code).dropna()
        return set(codes.unique())
    except: return set()

def fetch_financial_metrics(code):
    url = f"https://money.finance.sina.com.cn/corp/go.php/vFD_FinancialGuideLine/stockid/{code}/displaytype/4.phtml"
    try:
        response = requests.get(url, headers=get_random_header(), timeout=8)
        response.encoding = 'gb18030'
        if len(response.text) < 800: return None
        final_rows = []
        tables = pd.read_html(io.StringIO(response.text), header=None)
        for df in tables:
            if df.shape[1] < 2: continue
            if df.iloc[:, 0].astype(str).str.contains('æ¯è‚¡æ”¶ç›Š|å‡€èµ„äº§æ”¶ç›Šç‡', na=False).any():
                df = df.set_index(df.columns[0])
                df.index = df.index.astype(str).str.strip()
                raw_dates = df.iloc[0].astype(str)
                if raw_dates.str.contains('-|20', na=False).any():
                    df.columns = raw_dates
                    df = df.iloc[1:]
                df = df.loc[:, df.columns.notna()]
                cols = sorted(df.columns, key=lambda x: str(x), reverse=True)
                df = df[cols].iloc[:, :8]
                all_indices = df.index.astype(str)
                for keyword, short_name in INDICATOR_MAPPING.items():
                    candidates = all_indices[all_indices.str.contains(keyword, na=False)]
                    clean_candidates = [c for c in candidates if 'å¢é•¿ç‡' not in c and 'åŒæ¯”' not in c]
                    best_match = clean_candidates[0] if clean_candidates else None
                    if best_match:
                        row = df.loc[best_match].copy()
                        row.name = short_name
                        final_rows.append(row)
                break
        if not final_rows: return None
        result_df = pd.DataFrame(final_rows)
        result_df.insert(0, 'è‚¡ç¥¨ä»£ç ', code)
        result_df.index.name = 'æŒ‡æ ‡'
        result_df = result_df.reset_index()
        return result_df
    except: pass
    return None

def run_financial_crawler(target_codes):
    print(f"\nğŸš€ [é˜¶æ®µäºŒ] è´¢åŠ¡æ•°æ®æ™ºèƒ½è¡¥å½•...")
    done_codes = get_existing_financial_codes()
    target_codes_clean = [clean_code(c) for c in target_codes if clean_code(c)]
    todo_codes = [c for c in target_codes_clean if c not in done_codes]
    
    print(f"   å·²ç¼“å­˜: {len(done_codes)}, éœ€è¡¥å½•: {len(todo_codes)}")
    
    # åŒæ ·é™åˆ¶æ¯æ¬¡è¿è¡Œçš„è¡¥å½•æ•°é‡ï¼Œé˜²æ­¢GitHub Actionè¶…æ—¶ï¼ˆé€šå¸¸é™åˆ¶6å°æ—¶ï¼Œä½†æœ€å¥½æ§åˆ¶åœ¨30åˆ†é’Ÿå†…ï¼‰
    limit = 200 
    if len(todo_codes) > limit:
        print(f"   âš ï¸ å‰©ä½™ä»»åŠ¡è¾ƒå¤šï¼Œæœ¬æ¬¡åªå¤„ç†å‰ {limit} ä¸ªï¼Œç•™ç»™ä¸‹æ¬¡è‡ªåŠ¨è¿è¡Œ...")
        todo_codes = todo_codes[:limit]

    if not todo_codes:
        print("   âœ… è´¢åŠ¡æ•°æ®å·²æœ€æ–°ã€‚")
        return

    buffer = []
    try:
        for i, code in enumerate(todo_codes):
            print(f"   [{i+1}/{len(todo_codes)}] è´¢åŠ¡: {code} ... ", end="", flush=True)
            try:
                df = fetch_financial_metrics(code)
                if df is not None:
                    buffer.append(df)
                    print("âˆš")
                else:
                    print("x")
            except: print("x")
            
            if len(buffer) >= 5:
                pd.concat(buffer, ignore_index=True).to_csv(FINANCIAL_FILE, mode='a', index=False, header=not os.path.exists(FINANCIAL_FILE), encoding='utf-8-sig')
                buffer = []
            time.sleep(1.0)
            
        if buffer: 
            pd.concat(buffer, ignore_index=True).to_csv(FINANCIAL_FILE, mode='a', index=False, header=not os.path.exists(FINANCIAL_FILE), encoding='utf-8-sig')
    except KeyboardInterrupt: pass

# ================= é˜¶æ®µä¸‰ï¼šWebæ•°æ®ç”Ÿæˆ =================

def merge_and_export(market_df):
    print(f"\nğŸ§© [é˜¶æ®µä¸‰] ç”Ÿæˆ Web æ•°æ® (JSON)...")
    
    try:
        fin_df = pd.read_csv(FINANCIAL_FILE, dtype=str, on_bad_lines='skip')
        fin_df['è‚¡ç¥¨ä»£ç '] = fin_df['è‚¡ç¥¨ä»£ç '].apply(clean_code)
        fin_df = fin_df.drop_duplicates(subset=['è‚¡ç¥¨ä»£ç ', 'æŒ‡æ ‡'], keep='last')
        
        for col in fin_df.columns:
            if col not in ['è‚¡ç¥¨ä»£ç ', 'æŒ‡æ ‡']:
                fin_df[col] = pd.to_numeric(fin_df[col], errors='ignore')
                
        id_vars = [c for c in fin_df.columns if 'æŒ‡æ ‡' in c or 'ä»£ç ' in c]
        date_cols = [c for c in fin_df.columns if c not in id_vars]
        melted = fin_df.melt(id_vars=id_vars, value_vars=date_cols, var_name='æ—¥æœŸ', value_name='æ•°å€¼')
        melted = melted.dropna(subset=['æ•°å€¼'])
        
        indicator_col = next((c for c in id_vars if 'æŒ‡æ ‡' in c), None)
        pivot_df = melted.pivot_table(index='è‚¡ç¥¨ä»£ç ', columns=['æ—¥æœŸ', indicator_col], values='æ•°å€¼', aggfunc='first')
        
        # æ’åº
        sorted_cols = sorted(pivot_df.columns, key=lambda x: str(x[0]), reverse=True)
        pivot_df = pivot_df[sorted_cols]
        
    except Exception as e:
        print(f"   âš ï¸ è´¢åŠ¡æ•°æ®å¼‚å¸¸: {e}")
        pivot_df = pd.DataFrame()

    # ã€Webé€‚é…æ ¸å¿ƒã€‘ï¼šæ‰å¹³åŒ–åˆ—å
    if isinstance(pivot_df.columns, pd.MultiIndex):
        # å°† ('2023-12-31', 'EPS') å˜æˆ '2023-12-31_EPS'
        pivot_df.columns = [f"{col[0]}_{col[1]}" for col in pivot_df.columns]

    market_df['ä»£ç '] = market_df['ä»£ç '].apply(clean_code)
    market_df = market_df.set_index('ä»£ç ')
    
    # Join
    final_df = market_df.join(pivot_df, how='left')
    final_df = final_df.reset_index()

    # æ›¿æ¢ NaN ä¸º None (JSONæ ‡å‡†)
    final_df = final_df.where(pd.notnull(final_df), None)
    
    # å†™å…¥ JSON
    print(f"   æ­£åœ¨å†™å…¥ JSON: {JSON_FILE} ...")
    final_df.to_json(JSON_FILE, orient='records', force_ascii=False)
    print(f"ğŸ‰ JSON æ•°æ®å·²ç”Ÿæˆï¼å¤§å°: {os.path.getsize(JSON_FILE)/1024/1024:.2f} MB")

def main():
    print("="*60)
    print("      ğŸ“ˆ Aè‚¡å…¨å¸‚åœº Web ç‰ˆæ•°æ®ç”Ÿæˆå™¨")
    print("="*60)
    try:
        market_df = fetch_market_snapshot()
        if not market_df.empty:
            market_df = augment_with_monthly_prices(market_df)
            run_financial_crawler(market_df['ä»£ç '].tolist())
            merge_and_export(market_df)
        else:
            print("âŒ è¡Œæƒ…è·å–å¤±è´¥ã€‚")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        traceback.print_exc()

if __name__ == '__main__':
    main()